{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"bert_clustering2.ipynb","provenance":[{"file_id":"1HUpoCRk54rnHRc8-WoX_ck1miraDHWej","timestamp":1616966366528}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"capable-nigeria"},"source":["# BERT Clustering\n","\n","Unsupervised clustering of forum posts using the text body. A simple approach to try and find similar posts without strict classification.\n","\n","Right now the model doesn't do anything, which is very sad."],"id":"capable-nigeria"},{"cell_type":"markdown","metadata":{"id":"ShL07NMHZPt2"},"source":["References\n","\n","1. [Topic Modeling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)\n","2. [Multi-label Text Classification using BERT - The Mighty Transformer](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)\n","3. [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning)\n","4. [Usering BERT For Classifying Documents with Long Texts](https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d)"],"id":"ShL07NMHZPt2"},{"cell_type":"code","metadata":{"id":"significant-encounter"},"source":["!pip install transformers -q\n","\n","import re\n","import math\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from transformers import AutoTokenizer, AutoModel\n","from tokenizers.normalizers import BertNormalizer"],"id":"significant-encounter","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"devoted-spending"},"source":["np.random.RandomState(123)\n","tqdm.pandas()\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    print(f'using {torch.cuda.get_device_name(0)}')\n","else:\n","    print('no GPU avaiable')"],"id":"devoted-spending","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sexual-parliament"},"source":["## Load Data"],"id":"sexual-parliament"},{"cell_type":"code","metadata":{"id":"dominant-vision"},"source":["df_ori = pd.read_json('https://github.com/charlotte-zhuang/forum-recommender/blob/master/data/sitepoint.json?raw=true')\n","df_ori.head()"],"id":"dominant-vision","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"moral-excess"},"source":["## Preprocessing"],"id":"moral-excess"},{"cell_type":"code","metadata":{"id":"infectious-company"},"source":["def pre_tokenize_post(text_l: list) -> str:\n","    '''Converts a list of text into a single string that\n","        can be tokenized.\n","    '''\n","    \n","    text = ' '.join(text_l)\n","    \n","    # remove links\n","    text = re.sub(r'https?:\\/\\/[^\\s]*', r'', text)\n","    \n","    # remove closed single quotes\n","    text = re.sub(r\"‘([^‘’]*)’\", r'\\1', text)\n","\n","    # remove unwanted characters\n","    text = re.sub(r\"[^\\w\\s’]+\", r' ', text)\n","    \n","    # turn whitespace into a space\n","    text = re.sub(r'\\s+', r' ', text)\n","\n","    # lowercase\n","    text = text.lower()\n","    \n","    return text"],"id":"infectious-company","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"theoretical-responsibility"},"source":["df = df_ori.drop(labels=['title', 'tags'], axis=1)\n","df['post'] = df['post'].apply(pre_tokenize_post)\n","\n","for i in range(5):\n","    print(df.iat[i, 1], end='\\n\\n')"],"id":"theoretical-responsibility","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"continuous-vanilla"},"source":["# optionally drop data for convenience\n","# df = df[:1000]"],"id":"continuous-vanilla","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"better-functionality"},"source":["## Tokenize"],"id":"better-functionality"},{"cell_type":"code","metadata":{"id":"JGhHm-ss4g4l"},"source":["tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","tokenizer.normalizer = BertNormalizer()"],"id":"JGhHm-ss4g4l","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LwM2rRx5qLi"},"source":["Split samples up to avoid truncation, then tokenize each sample."],"id":"5LwM2rRx5qLi"},{"cell_type":"code","metadata":{"id":"WB3DioksvdZH"},"source":["def tokenize_text(text: str) -> list:\n","    '''Tokenizes text using chunks of up to 300 words. This does the exact\n","        same thing as encode_text(), but step by step.\n","\n","    Returns:\n","        numpy.ndarray: 2D array of IDs: axis0=chunks, axis1=tokens.\n","        numpy.ndarray: 2D array of attention masks: axis0=chunks, axis1=mask.\n","    '''\n","\n","    # break up sentences into chunks of 300 words with 50 overlapping\n","    res = []\n","    text = text.split()\n","    n = int(math.ceil((len(text) - 100) / 200))\n","\n","    res.append(' '.join(text[:300]))\n","\n","    for i in range(1, n):\n","        res.append(' '.join(text[i * 250 : i * 250 + 300]))\n","\n","    # tokenize\n","    res = list(map(lambda x: ['[CLS]'] + tokenizer.tokenize(x), res))\n","\n","    # convert to ids\n","    res = list(map(tokenizer.convert_tokens_to_ids, res))\n","\n","    # pad sentences to be 512 characters\n","    res = np.array(list(map(lambda x: x + [0] * (512 - len(x)), res)))\n","\n","    # attention mask\n","    attention_mask = np.where(res != 0, 1, 0)\n","\n","    return res , attention_mask"],"id":"WB3DioksvdZH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ElRx0G71ee-"},"source":["def encode_text(text):\n","    '''Tokenizes text using chunks of up to 300 words. This does the exact\n","        same thing as tokenize_text(), using the encode() method instead.\n","\n","    Returns:\n","        numpy.ndarray: 2D array of IDs: axis0=chunks, axis1=tokens.\n","        numpy.ndarray: 2D array of attention masks: axis0=chunks, axis1=mask.\n","    '''\n","    \n","\n","    # break up sentences into chunks of 300 words with 50 overlapping\n","    res = []\n","    text = text.split()\n","    n = int(math.ceil((len(text) - 100) / 200))\n","\n","    res.append(' '.join(text[:300]))\n","\n","    for i in range(1, n):\n","        res.append(' '.join(text[i * 250 : i * 250 + 300]))\n","    \n","    res = np.array(res)\n","\n","    # tokenize\n","    res = list(map(\n","        lambda x: tokenizer.encode(x, add_special_tokens=True),\n","        res))\n","\n","    # pad sentences to be 512 characters\n","    res = np.array(list(map(lambda x: x + [0] * (512 - len(x)), res)))\n","    \n","    # mask\n","    attention_mask = np.where(res != 0, 1, 0)\n","    \n","    return res , attention_mask"],"id":"_ElRx0G71ee-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaaVujG5-yEd"},"source":["tokens = list(map(encode_text, df['post']))\n","df['tokens'] = [x[0] for x in tokens]\n","df['mask'] = [x[1] for x in tokens]\n","df.head()"],"id":"YaaVujG5-yEd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9oB6yzGLGTn7"},"source":["## Classification\n","\n","I'm actually not using the BERT classifier anymore."],"id":"9oB6yzGLGTn7"},{"cell_type":"code","metadata":{"id":"blind-clock"},"source":["bert_model = AutoModel.from_pretrained('distilbert-base-uncased')"],"id":"blind-clock","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tJMzlg0G9V_"},"source":["Get class embeddings. Each chunk from a sample gets a vector of 768 embeddings (from the 768 hidden layers)."],"id":"3tJMzlg0G9V_"},{"cell_type":"code","metadata":{"id":"iK7dILUeHIsm"},"source":["def embed_tokens(sample):\n","\n","    tokens = torch.tensor(sample['tokens'])\n","    attention_mask = torch.tensor(sample['mask'])\n","\n","    with torch.no_grad():\n","        last_hidden_states = bert_model(tokens, attention_mask=attention_mask)\n","\n","    embeddings = last_hidden_states[0][:,0,:].numpy()\n","\n","    return embeddings"],"id":"iK7dILUeHIsm","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XB0Vx_zDTnDe"},"source":["This step takes a while."],"id":"XB0Vx_zDTnDe"},{"cell_type":"code","metadata":{"id":"ObD0tiJOJueX"},"source":["df['embed'] = df.progress_apply(embed_tokens, axis=1)"],"id":"ObD0tiJOJueX","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhSL11Z1KLPf"},"source":["df.head()"],"id":"BhSL11Z1KLPf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"irrAiYRLcqeR"},"source":[""],"id":"irrAiYRLcqeR","execution_count":null,"outputs":[]}]}